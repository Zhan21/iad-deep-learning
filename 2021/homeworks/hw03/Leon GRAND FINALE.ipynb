{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Трансформеры\n",
    "В этом домашнем задании мы рассмотим использование трансформеров в библиотеке PyTorch. Рассмотрим задачу языкового моделирования. Попробуем генерировать текст нейронной сетью. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на данные - https://drive.google.com/drive/folders/1x1A4ElliUGBPnHladGMwPxPuGxI8Vnpu?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:10:10.538325Z",
     "iopub.status.busy": "2021-12-20T10:10:10.537560Z",
     "iopub.status.idle": "2021-12-20T10:10:10.543022Z",
     "shell.execute_reply": "2021-12-20T10:10:10.542244Z",
     "shell.execute_reply.started": "2021-12-20T10:10:10.538275Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# хороший тон, импортировать все необходимые библиотеки в одной ячейке ;)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое языковое моделирование? Это предсказание вероятности следующего токена (слова или буквы) на основе предыдущих токенов. Математически это можно описать так:\n",
    "\n",
    "$$P(x_i|x_1, x_2 , ... , x_{i-1})$$ \n",
    "\n",
    "Последовательность $$ x_1, x_2, ... x_{i-1} $$ называют контекстом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 0 (0 баллов, но сделать нужно)\n",
    "Проставьте знаки неравенств, исходя из вашего опыта:\n",
    "$$ P(раму | мама, мыла) > P(папу | мама, мыла) $$\n",
    "$$ P(столу | дорога, ложка, к) < P(обеду | дорога, ложка, к) $$\n",
    "$$ P(Евпатий | меня, зовут) < P(Ваня | меня, зовут) $$\n",
    "$$ P(журналы | я, часто ,читаю) > P(комиксы | я, часто ,читаю) $$\n",
    "Попробуйте объяснить выбор для каждого из примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ :\n",
    "- встретить слово рама среди слов мама и мыла вероятнее, так как это поговорка или что-то такое\n",
    "- такой же принцип. Дорога ложко к обеду – фразеологизм\n",
    "- Ваня – более распространенное имя, чем Евпатий\n",
    "- не читаю комиксы, поэтому скажу, что тут знак \"больше\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если для некоторых из примеров проставить знаки достаточно просто, то на некоторые сложно сказать, какой овтет верный. Мы принимаем решение для данного задания исходя их опыта использования русского языка. Мы много читали на русском и слушали огромное количество русской речи. Обучение языковых моделей происходит по схожему принципу. \n",
    "\n",
    "Мы хотим показать модели столько текстов, сколько можем и надеемся, что она наберется достаточно опыта, чтобы расставлять такие знаки неравества максимально схоже с человеком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "Мы будем обучать языковую модель для предсказания следущей буквы. Такие языковые модели применяются в распозновании речи, так как предоставляют дополнительную информацию акустической модели при выборе следующего символа. Для начала, откройте файл с данными, посмотрите, какие символы входят в тексты, сколько их. Уберите из текста все символы переноса на новую строку и табуляцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:54.842205Z",
     "iopub.status.busy": "2021-12-20T10:06:54.841972Z",
     "iopub.status.idle": "2021-12-20T10:06:55.717755Z",
     "shell.execute_reply": "2021-12-20T10:06:55.717061Z",
     "shell.execute_reply.started": "2021-12-20T10:06:54.842173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../input/hw3transformer/small_corp_for_test.txt'\n",
    "file = open(path, 'r')\n",
    "data = file.readlines()\n",
    "file.close()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:55.719569Z",
     "iopub.status.busy": "2021-12-20T10:06:55.719183Z",
     "iopub.status.idle": "2021-12-20T10:06:55.908718Z",
     "shell.execute_reply": "2021-12-20T10:06:55.907989Z",
     "shell.execute_reply.started": "2021-12-20T10:06:55.719534Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [text.strip('\\n') for text in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (0.5 балла)\n",
    "Для обучения модели требуется сначала подготовить текст в подходящий для нейросети вид. Важно также отметить, что нужно добавить два токена start и end, которые отвечают за начало и конец текста. Используйте [ и ] для этой задачи. Также нам нужен токен pad, чтобы заполнять им текст до требуемой длинны для формирования батча.\n",
    "\n",
    "Реализуйте метод preprocess класса Preprocessor. Он должен принимать на вход текст и длинну текста, которую мы ожидаем получить на выходе. Текст должен быть переведен в нижний регистр, в конец текста добавляется требуемое число pad токенов, далее текст векторизуется (каждому символу ставится свое число). Вернуть требуется два вектора. Полученный результат без последнего токена (на нем будем обучаться) и полученный результат без первого токена (целевые метки при обучении)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:55.911394Z",
     "iopub.status.busy": "2021-12-20T10:06:55.910969Z",
     "iopub.status.idle": "2021-12-20T10:06:55.936640Z",
     "shell.execute_reply": "2021-12-20T10:06:55.935928Z",
     "shell.execute_reply.started": "2021-12-20T10:06:55.911357Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.alphabet = '_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '\n",
    "        self.token2ind = {}\n",
    "        self.ind2token = {}\n",
    "        for i in range(len(self.alphabet)):\n",
    "            self.token2ind[self.alphabet[i]] = i\n",
    "            self.ind2token[i] = self.alphabet[i]\n",
    "        \n",
    "    \n",
    "    def preprocess(self, text, window_size):\n",
    "        text = str.lower(text)\n",
    "        pads = window_size - len(text)\n",
    "        text = f'[{text}]'\n",
    "        text += '_' * pads\n",
    "        v1 = [self.token2ind[char] for char in text[1:]]\n",
    "        v2 = [self.token2ind[char] for char in text[:-1]]\n",
    "        return [v1, v2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла)\n",
    "Так как мы решили, что текст будет начинаться токеном [ и заканчиваться токеном ], данные нужно поправить. Реализуйте эту идею, добавьте данные токены в ваши тексты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "это задание сделано в прошлом пункте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "Так как мы не располагаем большими мощностями, то давайте ограничим максимальную длинну текста. Вы можете менять этот порог и тем самым уменьшать кол-во текстов в вашей выборке и увеличивая тем самым скорость обучения. Начнем же мы с 128. \n",
    "Выберите порог и оставьте только те тексты, длина которых не превосходит данный порог.\n",
    "\n",
    "Далее разбейте тексты на train и test, перемешайте тексты при разбиении, размер тестовой выборки должен быть 15% от общего числа текстов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:55.938174Z",
     "iopub.status.busy": "2021-12-20T10:06:55.937866Z",
     "iopub.status.idle": "2021-12-20T10:06:55.978199Z",
     "shell.execute_reply": "2021-12-20T10:06:55.977228Z",
     "shell.execute_reply.started": "2021-12-20T10:06:55.938136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:55.979653Z",
     "iopub.status.busy": "2021-12-20T10:06:55.979417Z",
     "iopub.status.idle": "2021-12-20T10:06:56.093078Z",
     "shell.execute_reply": "2021-12-20T10:06:56.092398Z",
     "shell.execute_reply.started": "2021-12-20T10:06:55.979617Z"
    }
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 127\n",
    "\n",
    "data = [text for text in data if len(text) <= THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:56.094611Z",
     "iopub.status.busy": "2021-12-20T10:06:56.094374Z",
     "iopub.status.idle": "2021-12-20T10:06:57.135628Z",
     "shell.execute_reply": "2021-12-20T10:06:57.134925Z",
     "shell.execute_reply.started": "2021-12-20T10:06:56.094578Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(data, random_state=777, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (2 балла)\n",
    "Напишем датасет. На вход датасету передается набор текстов, объект класса Preprocessor и размер окна, который вы выбрали в прошлом задании.\n",
    "Реализуйте методы __len__ и __getitem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:57.137302Z",
     "iopub.status.busy": "2021-12-20T10:06:57.137033Z",
     "iopub.status.idle": "2021-12-20T10:06:57.143416Z",
     "shell.execute_reply": "2021-12-20T10:06:57.142732Z",
     "shell.execute_reply.started": "2021-12-20T10:06:57.137266Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, preproc, win_size = 127):\n",
    "        self.x = x\n",
    "        self.preproc = preproc\n",
    "        self.win_size = win_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.preproc.preprocess(self.x[idx], self.win_size)\n",
    "        x = torch.LongTensor(x)\n",
    "        y = torch.LongTensor(y)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:57.145218Z",
     "iopub.status.busy": "2021-12-20T10:06:57.144733Z",
     "iopub.status.idle": "2021-12-20T10:06:57.154350Z",
     "shell.execute_reply": "2021-12-20T10:06:57.153638Z",
     "shell.execute_reply.started": "2021-12-20T10:06:57.145170Z"
    }
   },
   "outputs": [],
   "source": [
    "preproc = Preprocessor()\n",
    "train_dataset = TextDataset(data_train, preproc)\n",
    "test_dataset = TextDataset(data_test, preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (2 балла)\n",
    "Напишем модель. Класс для реализации positional encoding реализован за вас, он нужен, чтобы модель могла после получения эмбедингов понимать, на каком месте какой токен находится.\n",
    "\n",
    "Заполните пропуски в классе модели. Гипперпараметры модели вам предлагается подобрать самостоятельно. Рекомендуется использовать не более 6 слоев в трансформере. В декореде испоьлзуйте две линейных слоя с функцией активации ReLU между ними.\n",
    "\n",
    "## Задание 6_1 (0 баллов, но надо ответить!)\n",
    "При обучении языковой модели на основе трансформеров мы используем маскирование символов (как мы это делаем - уже реализовано). Напишите, почему мы это делаем? Почему это так важно?\n",
    "\n",
    "- Это важно, так как модель предсказывает выход для каждого символа. Не скрывая следующий вход данных модель будет подсматривать вперед и в итоге переобучится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:57.158977Z",
     "iopub.status.busy": "2021-12-20T10:06:57.158766Z",
     "iopub.status.idle": "2021-12-20T10:06:57.193768Z",
     "shell.execute_reply": "2021-12-20T10:06:57.193079Z",
     "shell.execute_reply.started": "2021-12-20T10:06:57.158955Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:57.195253Z",
     "iopub.status.busy": "2021-12-20T10:06:57.194913Z",
     "iopub.status.idle": "2021-12-20T10:06:57.205295Z",
     "shell.execute_reply": "2021-12-20T10:06:57.204550Z",
     "shell.execute_reply.started": "2021-12-20T10:06:57.195193Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:06:57.208997Z",
     "iopub.status.busy": "2021-12-20T10:06:57.208214Z",
     "iopub.status.idle": "2021-12-20T10:06:57.218921Z",
     "shell.execute_reply": "2021-12-20T10:06:57.218163Z",
     "shell.execute_reply.started": "2021-12-20T10:06:57.208961Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dropout=0.1):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, 32, padding_idx=37)\n",
    "        self.pe = PositionalEncoding(32, dropout)\n",
    "        self.transformer_encoder_layer = TransformerEncoderLayer(32, 8)\n",
    "        self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layer, 6)\n",
    "        self.decoder = nn.Sequential(nn.Linear(32, 64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(64, vocab_size))\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.pe.forward(self.emb(x)) # emb, then pe\n",
    "        x = x.transpose(1, 0)\n",
    "        x = self.transformer_encoder(x, src_mask) # transformer encoder with mask\n",
    "        x = self.decoder(x) # decoder\n",
    "        return x.transpose(1, 0)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # А вот и то самое маскирование\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2,5 балла)\n",
    "Финишная прямая. Давайте реализуем класс для обучения модели и ее валидации. Следуйте указаниям в коде и заполните недостающие фрагменты в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:29:57.372571Z",
     "iopub.status.busy": "2021-12-20T10:29:57.372137Z",
     "iopub.status.idle": "2021-12-20T10:29:57.392429Z",
     "shell.execute_reply": "2021-12-20T10:29:57.391561Z",
     "shell.execute_reply.started": "2021-12-20T10:29:57.372534Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, train_dataset, test_dataset):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.train_batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        \n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=self.train_batch_size)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=self.test_batch_size)\n",
    "        self.train_dataloader_size = len(train_dataset)\n",
    "        self.test_dataloader_size = len(test_dataset)\n",
    "        \n",
    "        self.device = 'cuda:0'\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0) # используйте CrossEntrophyLoss, передайте в качетсве параметра \n",
    "                                                             # ignore index индекс символа _, чтобы модель не штрафовалась за то\n",
    "                                                             # что идет после закрывающего токена\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=1e-08)\n",
    "        \n",
    "        self.steps_to_print = 1000\n",
    "        \n",
    "    def train_one_epoch(self, epoch_number):\n",
    "        step = 0\n",
    "        counted_loss = 0\n",
    "        current_time = time.time()\n",
    "        it = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_dataloader):\n",
    "            x, y = batch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            src_mask = self.model.generate_square_subsequent_mask(128).to(self.device)\n",
    "            y_pred = self.model(x, src_mask)\n",
    "            y_pred = torch.reshape(y_pred, (y_pred.shape[0]*128, 38))\n",
    "            y = y.flatten()\n",
    "            loss = self.criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            counted_loss = float(loss)\n",
    "            step += 1\n",
    "            it += 1\n",
    "            \n",
    "            \n",
    "            if step % self.steps_to_print == 0:\n",
    "                result = 'Train epoch '+str(epoch_number)+' | '\n",
    "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
    "                result += 'Counted loss '+str(counted_loss)+' | '\n",
    "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
    "                result += 'time '+str(time.time() - current_time) + ' | '\n",
    "                print(result)\n",
    "                current_time = time.time()\n",
    "                counted_loss = 0\n",
    "                it = 0\n",
    "    \n",
    "    def validate_one_epoch(self, epoch_number):\n",
    "        step = 0\n",
    "        counted_loss = 0\n",
    "        current_time = time.time()\n",
    "        it = 0\n",
    "        for batch in tqdm(self.test_dataloader):\n",
    "            x, y = batch\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            src_mask = self.model.generate_square_subsequent_mask(128).to(self.device)\n",
    "            y_pred = self.model(x, src_mask)\n",
    "            y_pred = torch.reshape(y_pred, (y_pred.shape[0]*128, 38))\n",
    "            y = y.flatten()\n",
    "            loss = self.criterion(y_pred, y)\n",
    "            counted_loss = float(loss)\n",
    "            step += 1\n",
    "            it += 1\n",
    "            \n",
    "            \n",
    "            if step%(self.steps_to_print // 2) == 0:\n",
    "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
    "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
    "                result += 'Counted loss '+str(counted_loss)+' | '\n",
    "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
    "                result += 'time '+str(time.time() - current_time) + ' | '\n",
    "                print(result)\n",
    "                current_time = time.time()\n",
    "                counted_loss = 0\n",
    "                it = 0\n",
    "        \n",
    "    def train(self, number_of_epochs):\n",
    "        model.to(self.device)\n",
    "        for epoch in range(1, number_of_epochs+1):\n",
    "            model.train()\n",
    "            self.train_one_epoch(epoch)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                self.validate_one_epoch(epoch)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое ppl? Перплексия. Ее можно интерпретировать как меру \"удивленности\" модели нужному символу. Чем меньше данная величина, тем лучше, ведь это значит, что модель если и сделала неправильный выбор, то не сильно удивлена своей ошибке.\n",
    "\n",
    "Проведите несколько экспериментов, посмотрите, при каких гипперпараметрах значение перплексии минимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (0.5 балла)\n",
    "Запустите обучение на нескольких эпохах. Ориентируйтесь на ваши вычислительные мощности и время работы. Вы всегда можете посчитать, сколько секунд уходит на один батч."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:29:58.064344Z",
     "iopub.status.busy": "2021-12-20T10:29:58.063828Z",
     "iopub.status.idle": "2021-12-20T10:29:58.080965Z",
     "shell.execute_reply": "2021-12-20T10:29:58.080305Z",
     "shell.execute_reply.started": "2021-12-20T10:29:58.064308Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len('_добсркгаупитнезчмфяжлйвцыэь-шхющёъ][ '), dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:29:58.309961Z",
     "iopub.status.busy": "2021-12-20T10:29:58.309666Z",
     "iopub.status.idle": "2021-12-20T10:29:58.316751Z",
     "shell.execute_reply": "2021-12-20T10:29:58.315952Z",
     "shell.execute_reply.started": "2021-12-20T10:29:58.309932Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T11:34:35.178673Z",
     "iopub.status.busy": "2021-12-20T11:34:35.178099Z",
     "iopub.status.idle": "2021-12-20T11:51:43.695450Z",
     "shell.execute_reply": "2021-12-20T11:51:43.694743Z",
     "shell.execute_reply.started": "2021-12-20T11:34:35.178633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158b0bf8bd2142ed9b8a97db3921cc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 | Step 1000/581190 | Counted loss 0.02129022218286991 | ppl 1.0000212904488213 | time 53.958184003829956 | \n",
      "Train epoch 1 | Step 2000/581190 | Counted loss 0.011111130937933922 | ppl 1.0000111111926668 | time 53.66317272186279 | \n",
      "Train epoch 1 | Step 3000/581190 | Counted loss 0.014386584982275963 | ppl 1.0000143866884696 | time 53.65493154525757 | \n",
      "Train epoch 1 | Step 4000/581190 | Counted loss 0.014124536886811256 | ppl 1.0000141246366385 | time 53.707499265670776 | \n",
      "Train epoch 1 | Step 5000/581190 | Counted loss 0.02209186740219593 | ppl 1.0000220921114293 | time 53.66835927963257 | \n",
      "Train epoch 1 | Step 6000/581190 | Counted loss 0.018644412979483604 | ppl 1.0000186445867876 | time 53.657555103302 | \n",
      "Train epoch 1 | Step 7000/581190 | Counted loss 0.020717110484838486 | ppl 1.0000207173250857 | time 53.724501848220825 | \n",
      "Train epoch 1 | Step 8000/581190 | Counted loss 0.010232968255877495 | ppl 1.0000102330206129 | time 53.56143355369568 | \n",
      "Train epoch 1 | Step 9000/581190 | Counted loss 0.012969533912837505 | ppl 1.0000129696180176 | time 53.55813407897949 | \n",
      "Validate epoch 1 | Step 500/102564 | Counted loss 0.01063968613743782 | ppl 1.0000212795986823 | time 8.15483283996582 | \n",
      "Validate epoch 1 | Step 1000/102564 | Counted loss 0.0033523316960781813 | ppl 1.0000067046858685 | time 8.18743109703064 | \n",
      "Validate epoch 1 | Step 1500/102564 | Counted loss 0.009211902506649494 | ppl 1.0000184239747327 | time 8.204453468322754 | \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee8bad1c1f24fd09a08d37236a4b4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 2 | Step 1000/581190 | Counted loss 0.0135105075314641 | ppl 1.0000135105987988 | time 53.84310293197632 | \n",
      "Train epoch 2 | Step 2000/581190 | Counted loss 0.009610163979232311 | ppl 1.000009610210157 | time 53.70690989494324 | \n",
      "Train epoch 2 | Step 3000/581190 | Counted loss 0.003650265745818615 | ppl 1.0000036502724081 | time 53.72431015968323 | \n",
      "Train epoch 2 | Step 4000/581190 | Counted loss 0.011499692685902119 | ppl 1.0000114997588077 | time 53.71136140823364 | \n",
      "Train epoch 2 | Step 5000/581190 | Counted loss 0.0072856987826526165 | ppl 1.0000072857253235 | time 53.57168483734131 | \n",
      "Train epoch 2 | Step 6000/581190 | Counted loss 0.01093230303376913 | ppl 1.0000109323627917 | time 53.59477686882019 | \n",
      "Train epoch 2 | Step 7000/581190 | Counted loss 0.005390532314777374 | ppl 1.0000053905468438 | time 53.81950759887695 | \n",
      "Train epoch 2 | Step 8000/581190 | Counted loss 0.0067556253634393215 | ppl 1.0000067556481826 | time 54.04359459877014 | \n",
      "Train epoch 2 | Step 9000/581190 | Counted loss 0.00696629798039794 | ppl 1.000006966322245 | time 53.6352174282074 | \n",
      "Validate epoch 2 | Step 500/102564 | Counted loss 0.010644015856087208 | ppl 1.000021288258304 | time 8.481247663497925 | \n",
      "Validate epoch 2 | Step 1000/102564 | Counted loss 0.005796188488602638 | ppl 1.000011592444169 | time 8.19894289970398 | \n",
      "Validate epoch 2 | Step 1500/102564 | Counted loss 0.010510762222111225 | ppl 1.000021021745398 | time 8.208348035812378 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "Итак, давайте попробуем погенерировать текст нашей сеткой. Закончите функцию по генерации текста. Попробуйте сгенерировать какой-нибудь текст. Помните, что если вы хотите генерировать текст с нуля, то вы должны передать в качестве текста только токен start.\n",
    "Прекратите генерировать текст, если модель выдала токен end или длинна текста больше 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T10:49:13.204526Z",
     "iopub.status.busy": "2021-12-20T10:49:13.204003Z",
     "iopub.status.idle": "2021-12-20T10:49:13.211419Z",
     "shell.execute_reply": "2021-12-20T10:49:13.210689Z",
     "shell.execute_reply.started": "2021-12-20T10:49:13.204487Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = model.generate_square_subsequent_mask(128).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T11:00:29.844685Z",
     "iopub.status.busy": "2021-12-20T11:00:29.844011Z",
     "iopub.status.idle": "2021-12-20T11:00:29.850518Z",
     "shell.execute_reply": "2021-12-20T11:00:29.849670Z",
     "shell.execute_reply.started": "2021-12-20T11:00:29.844642Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    x = []\n",
    "    device = 'cuda:0'\n",
    "    \n",
    "    for letter in text:\n",
    "        x.append(preproc.token2ind[letter])\n",
    "    x = torch.from_numpy(np.array(x))\n",
    "    return x\n",
    "    x = x.to(device)\n",
    "    \n",
    "    pred = model(x, )\n",
    "    ind = ...\n",
    "    \n",
    "    text += ... \n",
    "    \n",
    "    if ...:\n",
    "        return text\n",
    "    else:\n",
    "        return generate_text(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
